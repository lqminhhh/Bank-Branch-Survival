{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculate Branch Share, Market Share, and Network Equity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_networkequity(df, bank_name = \"Bank of America, National Association\", max_distances = 10):\n",
    "    df = df[\n",
    "        df['SIMS_LATITUDE'].notna() &\n",
    "        df['SIMS_LONGITUDE'].notna()\n",
    "    ]\n",
    "    \n",
    "    # Step 1: Filter BoA and non-BoA branches\n",
    "    boa_df = df.loc[df[\"NAMEFULL\"] == bank_name].copy()\n",
    "    non_boa_df = df.loc[~(df[\"NAMEFULL\"] == bank_name)].copy()\n",
    "\n",
    "    # Prepare coordinates for BoA and non-BoA branches\n",
    "    boa_coords = boa_df[['SIMS_LATITUDE', 'SIMS_LONGITUDE']].to_numpy()\n",
    "    non_boa_coords = non_boa_df[['SIMS_LATITUDE', 'SIMS_LONGITUDE']].to_numpy()\n",
    "\n",
    "    # Approximate conversion factor for lat/lon degrees to miles\n",
    "    # 1 degree latitude: 69 miles, 1 degree longitude: 54.6 miles at 40Â° latitude (rough average)\n",
    "    lat_to_miles = 69.0\n",
    "    lon_to_miles = 54.6\n",
    "\n",
    "    # Convert lat/lon to approximate miles for KDTree\n",
    "    boa_coords_miles = np.column_stack((boa_coords[:, 0] * lat_to_miles, boa_coords[:, 1] * lon_to_miles))\n",
    "    non_boa_coords_miles = np.column_stack((non_boa_coords[:, 0] * lat_to_miles, non_boa_coords[:, 1] * lon_to_miles))\n",
    "\n",
    "    # Build KDTree from non-BoA coordinates\n",
    "    tree = cKDTree(non_boa_coords_miles)\n",
    "\n",
    "    # Query how many non-BoA branches are within 'max_distances' miles for each BoA branch\n",
    "    radius = max_distances\n",
    "    non_boa_nearby_counts = tree.query_ball_point(boa_coords_miles, r=radius)\n",
    "    boa_df['Total Nearby Competitor Branches'] = [len(c) for c in non_boa_nearby_counts]\n",
    "\n",
    "    # Build KDTree from BoA coordinates only (converted to miles)\n",
    "    boa_tree = cKDTree(boa_coords_miles)\n",
    "\n",
    "    # Query how many BoA branches are within 'max_distances' miles for each BoA branch\n",
    "    # Subtract 1 to exclude the branch itself\n",
    "    boa_nearby_counts = boa_tree.query_ball_point(boa_coords_miles, r=radius)\n",
    "    boa_df['Nearby BoA Branches'] = [len(c) - 1 for c in boa_nearby_counts]\n",
    "\n",
    "    # Convert deposits column to numeric\n",
    "    boa_df['DEPSUMBR'] = pd.to_numeric(boa_df['DEPSUMBR'], errors='coerce')\n",
    "    non_boa_df['DEPSUMBR'] = pd.to_numeric(non_boa_df['DEPSUMBR'], errors='coerce')\n",
    "\n",
    "    boa_df['Total Nearby BoA Deposits'] = [\n",
    "        boa_df.iloc[[j for j in inds if j != i]]['DEPSUMBR'].sum()\n",
    "        for i, inds in enumerate(boa_nearby_counts)\n",
    "    ]\n",
    "\n",
    "    boa_df['Total Nearby Competitor Deposits'] = [\n",
    "        non_boa_df.iloc[inds]['DEPSUMBR'].sum()\n",
    "        for inds in non_boa_nearby_counts\n",
    "    ]\n",
    "\n",
    "    # Branch Share: (BoA branches) / (BoA branches + Non BoA branches)\n",
    "    boa_df['Branch Share'] = (boa_df['Nearby BoA Branches'] + 1) / (\n",
    "        boa_df['Nearby BoA Branches'] + boa_df['Total Nearby Competitor Branches'] + 1\n",
    "    )\n",
    "\n",
    "    # Market share: (nearby BoA deposits + this branch's deposit) / (total nearby BoA + competitor deposits + this branch's deposit)\n",
    "    boa_df['Market Share'] = (\n",
    "        boa_df['Total Nearby BoA Deposits'] + boa_df['DEPSUMBR']\n",
    "    ) / (\n",
    "        boa_df['Total Nearby BoA Deposits'] + boa_df['Total Nearby Competitor Deposits'] + boa_df['DEPSUMBR']\n",
    "    )\n",
    "\n",
    "    boa_df[\"Network Equity\"] = boa_df[\"Market Share\"] / boa_df[\"Branch Share\"]\n",
    "\n",
    "    return boa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANKNAME = \"Bank of America, National Association\"\n",
    "MAXDISTANCES = 10\n",
    "\n",
    "dfs_filtered = []\n",
    "\n",
    "for year in list(range(2014, 2025)):\n",
    "    data_path = f\"./data/metadata//FDIC_data/SOD{year}.csv\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    boa_df = caculate_networkequity(df, BANKNAME, MAXDISTANCES)\n",
    "    dfs_filtered.append(boa_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merge all data into one Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all years\n",
    "df_all = pd.concat(dfs_filtered, ignore_index=True)\n",
    "\n",
    "# Sort by branch and year\n",
    "df_all_sorted = df_all.sort_values(by=['BRNUM', 'YEAR'])\n",
    "\n",
    "# Calculate DEPSUMBR change\n",
    "df_all_sorted['DEPSUMBR_PREV'] = df_all_sorted.groupby('BRNUM')['DEPSUMBR'].shift(1)\n",
    "df_all_sorted['DEPSUMBR_CHANGE'] = (\n",
    "    (df_all_sorted['DEPSUMBR'] - df_all_sorted['DEPSUMBR_PREV']) / df_all_sorted['DEPSUMBR_PREV']\n",
    ")\n",
    "df_all_sorted['DEPSUMBR_CHANGE'] = df_all_sorted['DEPSUMBR_CHANGE'].fillna(0)\n",
    "\n",
    "# Get latest row per branch \n",
    "df_latest = df_all_sorted.drop_duplicates(subset='BRNUM', keep='last')\n",
    "df_latest['closed'] = (df_latest['YEAR'] < 2024).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merge with income data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#income data\n",
    "income2024 = pd.read_csv('./data/metadata//Income_Data/ACSST5Y2023.S1901-Data.csv')\n",
    "income2023 = pd.read_csv('./data/metadata//Income_Data/ACSST5Y2022.S1901-Data.csv')\n",
    "income2022 = pd.read_csv('./data/metadata//Income_Data/ACSST5Y2021.S1901-Data.csv')\n",
    "income2021 = pd.read_csv('./data/metadata//Income_Data/ACSST5Y2020.S1901-Data.csv')\n",
    "income2020 = pd.read_csv('./data/metadata//Income_Data/ACSST5Y2019.S1901-Data.csv')\n",
    "income2019 = pd.read_csv('./data/metadata//Income_Data/ACSST5Y2018.S1901-Data.csv')\n",
    "income2018 = pd.read_csv('./data/metadata//Income_Data/ACSST5Y2017.S1901-Data.csv')\n",
    "income2017 = pd.read_csv('./data/metadata//Income_Data/ACSST5Y2016.S1901-Data.csv')\n",
    "income2016 = pd.read_csv('./data/metadata//Income_Data/ACSST5Y2015.S1901-Data.csv')\n",
    "income2015 = pd.read_csv('./data/metadata//Income_Data/ACSST5Y2014.S1901-Data.csv')\n",
    "income2014 = pd.read_csv('./data/metadata//Income_Data/ACSST5Y2013.S1901-Data.csv')\n",
    "\n",
    "income_dfs = [\n",
    "    income2014, income2015, income2016, income2017, income2018,\n",
    "    income2019, income2020, income2021, income2022, income2023, income2024\n",
    "]\n",
    "income_years = list(range(2014, 2025)) \n",
    "df_latest['ZIPBR'] = df_latest['ZIPBR'].astype(str).str.zfill(5) \n",
    "df_latest = df_latest.drop(columns=['ZIP'], errors='ignore')\n",
    "cleaned_income_dfs = {}\n",
    "\n",
    "for df, year in zip(income_dfs, income_years):\n",
    "    df_clean = df.iloc[1:].copy()\n",
    "    df_clean = df_clean.rename(columns={'NAME': 'ZIP'})\n",
    "    df_clean['ZIP'] = df_clean['ZIP'].str.replace(\"ZCTA5 \", \"\").str.zfill(5)\n",
    "    df_clean['YEAR'] = year\n",
    "    cleaned_income_dfs[year] = df_clean\n",
    "\n",
    "merged_list = []\n",
    "\n",
    "for year in df_latest['YEAR'].unique():\n",
    "    df_year = df_latest[df_latest['YEAR'] == year]\n",
    "    income_df = cleaned_income_dfs.get(year)\n",
    "\n",
    "    if income_df is not None:\n",
    "        merged = df_year.merge(income_df, left_on=['ZIPBR', 'YEAR'], right_on=['ZIP', 'YEAR'], how='left')\n",
    "        merged_list.append(merged)\n",
    "    else:\n",
    "        merged_list.append(df_year)\n",
    "\n",
    "df_income_merged = pd.concat(merged_list, ignore_index=True)\n",
    "na_rows = df_income_merged[df_income_merged['ZIP'].isna()]\n",
    "df_income_merged = df_income_merged[df_income_merged['ZIP'].notna()] \n",
    "\n",
    "cols_to_keep = [\n",
    "    'S1901_C01_012E',\n",
    "    'S1901_C01_013E'\n",
    "]\n",
    "\n",
    "# Drop all columns that start with 'S1901' but are NOT in cols_to_keep\n",
    "df_income_merged = df_income_merged.drop(\n",
    "    columns=[col for col in df_income_merged.columns\n",
    "             if col.startswith('S1901') and col not in cols_to_keep]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_merged['S1901_C01_013E'] = pd.to_numeric(\n",
    "    df_income_merged['S1901_C01_013E'], errors='coerce'\n",
    ") \n",
    "df_income_merged['S1901_C01_012E'] = pd.to_numeric(\n",
    "    df_income_merged['S1901_C01_012E'], errors='coerce'\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rename columns and drop unecessary ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_merged = df_income_merged.drop(['UNINUMBR', 'ZIPBR', 'GEO_ID', 'Unnamed: 130','DEPSUMBR_PREV'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = {\n",
    "    'YEAR' : 'year', \n",
    "    'CERT' : 'FDIC_cert', \n",
    "    'NAMEFULL': \"bank_name\", \n",
    "    'ADDRESBR': \"adress\", \n",
    "    'BRNUM': \"branch_id\", \n",
    "    'STALPBR': \"state_abb\",\n",
    "    'BRSERTYP': \"branch_service_type\", \n",
    "    'CITYBR': \"city\", \n",
    "    'CNTYNAMB': \"county\", \n",
    "    'CSABR': \"combined_stat_area\", \n",
    "    'CSANAMBR': \"combined_stat_area_name\", \n",
    "    'DEPSUMBR': \"deposits\",\n",
    "    'METROBR': \"is_metropolitan\", \n",
    "    'MICROBR': \"is_micropolitan\", \n",
    "    'MSABR': \"metro_stat_area\", \n",
    "    'NAMEBR': \"branch_name\", \n",
    "    'SIMS_ESTABLISHED_DATE': \"established_date\",\n",
    "    'SIMS_LATITUDE': \"latitude\", \n",
    "    'SIMS_LONGITUDE': \"longitude\", \n",
    "    'STCNTYBR': \"state_county_num\", \n",
    "    'STNAMEBR': \"state_name\", \n",
    "    'Total Nearby Competitor Branches': \"total_nearby_competitor_branches\", \n",
    "    'Nearby BoA Branches': \"nearby_boa_branches\",\n",
    "    'Total Nearby BoA Deposits': \"total_nearby_boa_deposits\", \n",
    "    'Total Nearby Competitor Deposits': \"total_nearby_competitor_deposits\",\n",
    "    'Branch Share': \"branch_share\", \n",
    "    'Market Share': \"market_share\", \n",
    "    'Network Equity': \"network_equity\", \n",
    "    'ZIP': \"zipcode\", \n",
    "    'DEPSUMBR_CHANGE': \"deposit_change\",\n",
    "    'S1901_C01_012E': \"estimate_household_median_income\", \n",
    "    'S1901_C01_013E': \"estimate_household_mean_income\"\n",
    "}\n",
    "\n",
    "df_final= df_income_merged.rename(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store final data\n",
    "df_final.to_csv('data/cleaned_data/cleaned_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
