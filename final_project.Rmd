---
title: "Final Project"
output: html_document
date: "2025-05-04"
output: 
  html_document:
    df_print: paged
    code_folding: show
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

```{r}
library(tidyverse)
library(caret)
library(dplyr)
library(ggthemes)
library(glmnet)
library(MASS)
library(class)
library(pROC)
library(randomForest)
library(ggplot2)
library(corrplot)
library(GGally)
library(rpart)
library(rpart.plot)
library(xgboost)
library(gbm)
library(doParallel)
boa_data <- read.csv("data/cleaned_data/cleaned_data.csv")
boa_data$closed <- as.factor(boa_data$closed)
```

```{r}
boa_data <- boa_data %>%
  mutate(region_code = case_when(
    is_metropolitan == 1 ~ 3,
    is_micropolitan == 1 ~ 2,
    TRUE ~ 1
  ))
```


## I. Data Exploration

We want to take a look at the data first.

```{r}
glimpse(boa_data)
```

Now, we want to see within 10 year, how many BoA branches have closed and how many are still opened.

```{r}
boa_data %>%
  mutate(
    region = recode(factor(region_code),
                    `1` = "Rural",
                    `2` = "Micropolitan",
                    `3` = "Metropolitan"),
    branch_status = recode(factor(closed),
                           `0` = "Opened",
                           `1` = "Closed")
  ) %>%
  group_by(region, branch_status) %>%
  summarise(count = n(), .groups = "drop") %>%
  ggplot(aes(x = region, y = count, fill = branch_status)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
  geom_text(aes(label = count),
            position = position_dodge(width = 0.9),
            vjust = -0.3, size = 4) +
  labs(
    title = "Branch Status by Region Type",
    x = "Region Type",
    y = "Number of Branches",
    fill = "Branch Status"
  ) +
  theme_minimal() + 
  theme(
    legend.position = "top",
    plot.title = element_text(size = 14, face = "bold")
  ) 
```

```{r}
boa_data %>%
  dplyr::select(closed,
         deposits,
         estimate_household_mean_income,
         estimate_household_median_income,
         network_equity) %>%
  pivot_longer(-closed, names_to = "Variable", values_to = "Value") %>%
  mutate(
    Variable = recode(Variable,
      deposits = "Deposits",
      estimate_household_mean_income = "Estimate Household Mean Income",
      estimate_household_median_income = "Estimate Household Median Income",
      network_equity = "Network Equity"
    )
  ) %>%
  ggplot(aes(x = factor(closed), y = Value, fill = factor(closed))) +
  geom_boxplot(outlier.size = 0.8, outlier.alpha = 0.4) +
  facet_wrap(~ Variable, scales = "free_y") +
  scale_y_log10() + 
  labs(
    title = "Boxplots of Key Variables by Closure Status",
    x = "Branch Closure Status (0 = Opened, 1 = Closed)",
    y = "Value (Log 10)"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 14, face = "bold")
  )
```


## II. Statistical Analysis

```{r}
df_cleaned <- boa_data %>%
  dplyr::select(closed,
                combined_stat_area,
                deposits,
                branch_share,
                market_share,
                network_equity,
                deposit_change,
                nearby_boa_branches,
                total_nearby_competitor_branches,
                total_nearby_competitor_deposits,
                total_nearby_boa_deposits,
                estimate_household_median_income,
                estimate_household_mean_income,
                region_code) %>%
  filter(if_all(everything(), ~ !is.na(.) & !is.infinite(.)))
```

Note: 46 of the data points are removed due to the data either being infinite or missing.

```{r}
# Select only numeric columns for correlation
numeric_vars <- df_cleaned %>%
  dplyr::select(deposits,
                branch_share,
                market_share,
                network_equity,
                total_nearby_competitor_branches,
                total_nearby_competitor_deposits,
                estimate_household_median_income,
                estimate_household_mean_income)

cor_matrix <- cor(numeric_vars, use = "complete.obs")

corrplot(cor_matrix,
         method = "color",          
         type = "upper",            
         addCoef.col = "black",     
         number.cex = 0.7,          
         tl.col = "black",          
         tl.srt = 45,               
         tl.cex = 0.9,              
         col = colorRampPalette(c("blue", "white", "red"))(200),  
         mar = c(0, 0, 1, 0),       
         title = "Correlation Matrix of Key Predictors"
)

```

### 1. Supervised Learning

We will now split the data into training and testing data (70-30 split).

```{r}
set.seed(123)
train_index <- createDataPartition(df_cleaned$closed, p = 0.7, list = FALSE)
train_data <- df_cleaned[train_index, ]
test_data <- df_cleaned[-train_index, ]
```

#### 1.1. Decision Tree

We will begin by modeling a simple decision tree model.

```{r}
# Fit decision tree
tree_model <- rpart(closed ~ ., 
                    data = train_data, 
                    method = "class", 
                    control = rpart.control(minsplit = 10, cp = 0.01))
```

```{r}
# Predict on test set
pred_class <- predict(tree_model, newdata = test_data, type = "class")

# Confusion matrix and accuracy
conf_mat <- confusionMatrix(pred_class, test_data$closed)
print(conf_mat)
```

```{r}
# Plot the tree
rpart.plot(tree_model,
           type = 2,
           extra = 106,
           fallen.leaves = TRUE,
           box.palette = "Blues",
           main = "Decision Tree for Branch Closure")
```

We can see that most of the decisions being made in the tree are related to `deposit_change` and `deposit`. This suggests that the tree is biased towards these two features.

#### 1.2. Random Forest

To avoid the problem occured from decision tree, we will now test a more advanced model, an improved version of decision tree by adding randomness.

```{r}
# Train the Random Forest model
rf_model <- randomForest(closed ~ ., data = train_data, ntree = 100, importance = TRUE)

# Predict
predictions <- predict(rf_model, newdata = test_data)
```

```{r}
# Evaluate
confusionMatrix(predictions, test_data$closed)
```

```{r}
# Get variable importance
importance_df <- as.data.frame(importance(rf_model))
importance_df$Variable <- rownames(importance_df)


ggplot(importance_df, aes(x = reorder(Variable, MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Variable Importance (Mean Decrease Accuracy)",
       x = "Variable",
       y = "Importance") +
  theme_minimal()

```

#### 1.3. Boosting

Lastly, we want to try boosting method. We will begin by using a basic boosting model. 

```{r}
train_data$closed <- as.numeric(as.character(train_data$closed))

# Fit GBM model
model_boosting <- gbm(
  closed ~ .,
  data = train_data,
  distribution = "bernoulli",
  n.trees = 5000,
  shrinkage = 0.01,
  interaction.depth = 3,
  n.minobsinnode = 10,
  cv.folds = 10,
  verbose = FALSE
)
```


```{r}
best_iter <- gbm.perf(model_boosting, method = "cv")
```

```{r}
# Predict probabilities on test data
pred_probs <- predict(model_boosting, newdata = test_data, n.trees = best_iter, type = "response")

# Convert probabilities to class (e.g. 0.5 threshold)
pred_class <- ifelse(pred_probs > 0.5, 1, 0)

# Evaluate
confusionMatrix(factor(pred_class), factor(test_data$closed))
```

We will now try XGBoost. We will do cross validation to tune the parameters to optimize the boosting model. The main metric we will use is Sensitivity, to ensure we catch as many true branch closures as possible. In addition, we also want to avoid Type II error - situation when the branch is predicted to be active when it is actually closed.

```{r}
# Remove target column and convert predictors to matrix
X_train <- model.matrix(closed ~ . - 1, data = train_data)
y_train <- as.numeric(as.character(train_data$closed))

X_test <- model.matrix(closed ~ . - 1, data = test_data)
y_test <- as.numeric(as.character(test_data$closed))

# Create DMatrix for XGBoost
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)
```

```{r}
hyperparam_grid <- expand.grid(
  nrounds = seq(from = 200, to = 500, by = 100),
  eta = c(0.01, 0.1),
  max_depth = c(5, 6, 7),
  gamma = c(0, 1),
  colsample_bytree = c(0.5, 0.75, 1.0),
  min_child_weight = 1,
  subsample = 1
)

num_cores <- parallel::detectCores() - 1
cl <- makeCluster(num_cores)
registerDoParallel(cl)
```

```{r}
tune_control <- trainControl(
  method = "cv", 
  number = 10, 
  verboseIter = FALSE, 
  allowParallel = TRUE
)

xgb_cv <- train(
  closed ~ .,
  data = train_data,
  method = "xgbTree",
  trControl = tune_control,
  metric = "Sens",   # maximize recall
  tuneGrid = hyperparam_grid
)

stopCluster(cl)
registerDoSEQ()
```

After tuning the model, we will use the optimal parameters to fit the final XGBoost model.

```{r}
final_model <- xgboost(data = as.matrix(X_train),
                       label = y_train,
                       booster = "gbtree",
                       objective = "binary:logistic",
                       nrounds = xgb_cv$bestTune$nrounds,
                       max_depth=xgb_cv$bestTune$max_depth,
                       colsample_bytree=xgb_cv$bestTune$colsample_bytree,
                       min_child_weight=xgb_cv$bestTune$min_child_weight,
                       subsample=xgb_cv$bestTune$subsample,
                       eta=xgb_cv$bestTune$eta,
                       gamma=xgb_cv$bestTune$gamma,
                       scale_pos_weight = 0.5,
                       verbose = 0)

# now evaluate
y_pred <- predict(final_model, as.matrix(X_test), type = "response") 
# Convert probabilities to class (e.g. 0.5 threshold)
pred_class <- ifelse(y_pred > 0.5, 1, 0)

# Evaluate
confusionMatrix(factor(pred_class), factor(test_data$closed))
```

```{r}
importance_matrix <- xgb.importance(colnames(X_train), model = final_model)
xgb.plot.importance(importance_matrix, rel_to_first = TRUE, xlab = "Relative importance", main = "Feature Importance")
```


### 2. Survival Analysis